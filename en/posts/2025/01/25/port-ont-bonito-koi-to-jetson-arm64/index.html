<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Running Bonito Basecaller on Nvidia Jetson and other ARM64 devices | Sparktour’s Blog</title><meta name=keywords content="bonito"><meta name=description content="Jetson is an embedded AI computing platform launched by Nvidia. Compared to laptops and servers, Jetson has a smaller size and lower power consumption, making it suitable for deployment on edge devices. Oxford Nanopore Technology has also released MinION Mk1C based on the Jetson platform for real-time analysis of sequencing data. This article describes how to install and run Bonito Basecaller on Jetson for real-time sequencing data analysis and model training on edge devices."><meta name=author content="sparktour"><link rel=canonical href=https://blog.sparktour.me/en/posts/2025/01/25/port-ont-bonito-koi-to-jetson-arm64/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.sparktour.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.sparktour.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.sparktour.me/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.sparktour.me/apple-touch-icon.png><link rel=mask-icon href=https://blog.sparktour.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://blog.sparktour.me/posts/2025/01/25/port-ont-bonito-koi-to-jetson-arm64/><link rel=alternate hreflang=en href=https://blog.sparktour.me/en/posts/2025/01/25/port-ont-bonito-koi-to-jetson-arm64/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://blog.sparktour.me/scss/callout.css><link rel=stylesheet href=https://s4.zstatic.net/ajax/libs/disqusjs/3.0.2/styles/disqusjs.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-6EJ4YX1XZ8"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6EJ4YX1XZ8")}</script><meta property="og:url" content="https://blog.sparktour.me/en/posts/2025/01/25/port-ont-bonito-koi-to-jetson-arm64/"><meta property="og:site_name" content="Sparktour’s Blog"><meta property="og:title" content="Running Bonito Basecaller on Nvidia Jetson and other ARM64 devices"><meta property="og:description" content="Jetson is an embedded AI computing platform launched by Nvidia. Compared to laptops and servers, Jetson has a smaller size and lower power consumption, making it suitable for deployment on edge devices. Oxford Nanopore Technology has also released MinION Mk1C based on the Jetson platform for real-time analysis of sequencing data. This article describes how to install and run Bonito Basecaller on Jetson for real-time sequencing data analysis and model training on edge devices."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-25T21:00:00+00:00"><meta property="article:modified_time" content="2025-01-26T21:53:35+08:00"><meta property="article:tag" content="Bonito"><meta property="og:image" content="https://assets.sparktour.me/img/blog/2025/port-ont-bonito-koi-to-jetson-arm64/cover.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://assets.sparktour.me/img/blog/2025/port-ont-bonito-koi-to-jetson-arm64/cover.jpg"><meta name=twitter:title content="Running Bonito Basecaller on Nvidia Jetson and other ARM64 devices"><meta name=twitter:description content="Jetson is an embedded AI computing platform launched by Nvidia. Compared to laptops and servers, Jetson has a smaller size and lower power consumption, making it suitable for deployment on edge devices. Oxford Nanopore Technology has also released MinION Mk1C based on the Jetson platform for real-time analysis of sequencing data. This article describes how to install and run Bonito Basecaller on Jetson for real-time sequencing data analysis and model training on edge devices."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Running Bonito Basecaller on Nvidia Jetson and other ARM64 devices","item":"https://blog.sparktour.me/en/posts/2025/01/25/port-ont-bonito-koi-to-jetson-arm64/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Running Bonito Basecaller on Nvidia Jetson and other ARM64 devices","name":"Running Bonito Basecaller on Nvidia Jetson and other ARM64 devices","description":"Jetson is an embedded AI computing platform launched by Nvidia. Compared to laptops and servers, Jetson has a smaller size and lower power consumption, making it suitable for deployment on edge devices. Oxford Nanopore Technology has also released MinION Mk1C based on the Jetson platform for real-time analysis of sequencing data. This article describes how to install and run Bonito Basecaller on Jetson for real-time sequencing data analysis and model training on edge devices.","keywords":["bonito"],"articleBody":"Jetson is an embedded AI computing platform launched by Nvidia. Compared to laptops and servers, Jetson has a smaller size and lower power consumption, making it suitable for deployment on edge devices. Oxford Nanopore Technology has also released MinION Mk1C based on the Jetson platform for real-time analysis of sequencing data. This article describes how to install and run Bonito Basecaller on Jetson for real-time sequencing data analysis and model training on edge devices.\nThis article is based on the Jetson Xavier NX development board running Jetson Linux 35.2.1. Other Jetson devices may require minor adjustments.\n$ uname -r 5.10.216-tegra $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Sun_Oct_23_22:16:07_PDT_2022 Cuda compilation tools, release 11.4, V11.4.315 Build cuda_11.4.r11.4/compiler.31964100_0 Ensure CUDA Dependencies Bonito Basecaller is a deep learning model based on CUDA, so it is necessary to ensure that CUDA dependencies are installed. During the installation of Jetson Linux using sdkmanager, you need to select the installation of the CUDA Toolkit to ensure that CUDA dependencies are installed. After entering the system, you can check the CUDA version using the following command:\n$ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA Corporation Built on Sun_Oct_23_22:16:07_PDT_2022 Cuda compilation tools, release 11.4, V11.4.315 Build cuda_11.4.r11.4/compiler.31964100_0 Install Conda To avoid conflicts with the system’s Python environment, we can use Conda to create an independent Python environment. First, we create a Conda environment:\nconda create -n bonito-py38-081 conda activate bonito-py38-081 And install Python 3.8 in this environment:\nconda install python=3.8 Install Bonito Observing the requirements.txt file of Bonito, we can see that Bonito depends on the following Python libraries:\n# general requirements edlib fast-ctc-decode mappy networkx # required for py3.8 torch compatability numpy\u003c2 # numpy~=2 is currently unreleased, it may have breaking changes pandas\u003c3 # pandas~=3 is currently unreleased, it may have breaking changes parasail pod5 pysam python-dateutil requests toml tqdm wheel # specific requirements torch==2.1.2 # ont requirements ont-fast5-api ont-koi ont-remora After testing by the author, the packages that have compatibility issues on the arm64 architecture are mainly focused on torch and ont-koi. Among them, PyTorch requires the installation of the torch package provided by NVIDIA, and ont-koi requires the porting of the dorado koi library.\nPyTorch According to Nvidia’s documentation and compatibility matrix table, we can install version 2.1.0a which is closest to Pytorch 2.1.2 (Please select the appropriate PyTorch version according to the actual Jetpack Linux version):\n# Download the PyTorch wheel file wget https://developer.download.nvidia.com/compute/redist/jp/v512/pytorch/torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl # Install numpy and pandas pip install \"numpy\u003c2\" \"pandas\u003c3\" # Install PyTorch pip install torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl After installation, you can use the following command to determine that PyTorch has read the Jetson’s GPU:\npython3 \u003e\u003e\u003e import torch torch.cuda.get_device_name(0) \u003e\u003e\u003e torch.cuda.get_device_name(0) 'Xavier' ont-koi Compile so file Since ONT only provides the koi library for the x86_64 architecture, we need to port the koi library to the arm64 architecture ourselves. After observation by the author, pre-compiled koi packages were found in Dorado’s Koi.cmake. We can directly download and compile the so file. We first download the koi package from ONT:\n# Download the koi package wget https://cdn.oxfordnanoportal.com/software/analysis/libkoi-0.4.3-Linux-aarch64-cuda-11.4.tar.gz tar -zxvf libkoi-0.4.3-Linux-aarch64-cuda-11.4.tar.gz Then, create a setup.py in the folder:\nimport os from cffi import FFI ffi = FFI() def build_koi(): with open(\"include/koi.h\", 'r') as f: header_content = f.read() ffi.cdef(header_content) ffi.set_source( 'koi._runtime', '#include ', include_dirs=['./include'], # include library_dirs=['./lib'], # lib libraries=['koi'], # link libkoi.a extra_link_args=['-Wl,-rpath,$ORIGIN/lib'] ) ffi.compile() if __name__ == '__main__': build_koi() And run the script. After compilation, we can find the _runtime.cpython-38-aarch64-linux-gnu.so file in the current directory.\nThe author also provides a pre-compiled so file for readers to use: https://assets.sparktour.me/img/blog/2025/port-ont-bonito-koi-to-jetson-arm64/_runtime.cpython-38-aarch64-linux-gnu.so.tar.gz Install ont-koi Since the koi architecture is different, we need to install ont-koi manually:\n# We first download the x86_64 architecture whl package wget https://files.pythonhosted.org/packages/fe/68/4bb9241c65d6c0c51b6ae366b5dedda2e8a41ba17d2559079ee8811808ef/ont_koi-0.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl # Unzip the wheel file (a wheel is essentially a zip file) unzip ont_koi-0.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -d ont_koi_temp # Find the Python site-packages directory SITE_PACKAGES=$(python -c \"import site; print(site.getsitepackages()[0])\") # Copy the files directly to site-packages (but be sure to back up first) cp -r ont_koi_temp/koi $SITE_PACKAGES/ cp -r ont_koi_temp/ont_koi.libs $SITE_PACKAGES/ cp -r ont_koi_temp/ont_koi-0.4.4.dist-info $SITE_PACKAGES/ # Replace the so file cp /path/to/your/new/_runtime.cpython-38-aarch64-linux-gnu.so $SITE_PACKAGES/koi/_runtime.abi3.so Install Bonito We first download the Bonito source code:\ngit clone https://github.com/nanoporetech/bonito cd bonito Comment out the dependencies of torch and ont-koi in requirements.txt, and then install Bonito:\npip install . Run Bonito bonito basecaller dna_r9.4.1_e8_sup@v3.3 --reference reference.mmi /data/reads \u003e basecalls.bam ","wordCount":"726","inLanguage":"en","image":"https://assets.sparktour.me/img/blog/2025/port-ont-bonito-koi-to-jetson-arm64/cover.jpg","datePublished":"2025-01-25T21:00:00Z","dateModified":"2025-01-26T21:53:35+08:00","author":{"@type":"Person","name":"sparktour"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.sparktour.me/en/posts/2025/01/25/port-ont-bonito-koi-to-jetson-arm64/"},"publisher":{"@type":"Organization","name":"Sparktour’s Blog","logo":{"@type":"ImageObject","url":"https://blog.sparktour.me/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.sparktour.me/en/ accesskey=h title="Sparktour’s Blog (Alt + H)">Sparktour’s Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://blog.sparktour.me/ title=中文 aria-label=中文>中文</a></li></ul></div></div><ul id=menu><li><a href=https://blog.sparktour.me/en/archives title=Archive><span>Archive</span></a></li><li><a href=https://blog.sparktour.me/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.sparktour.me/en/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://blog.sparktour.me/en/about/ title=About><span>About</span></a></li><li><a href=https://blog.sparktour.me/en/friends/ title=Friends><span>Friends</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.sparktour.me/en/>Home</a></div><h1 class="post-title entry-hint-parent">Running Bonito Basecaller on Nvidia Jetson and other ARM64 devices</h1><div class=post-description>Jetson is an embedded AI computing platform launched by Nvidia. Compared to laptops and servers, Jetson has a smaller size and lower power consumption, making it suitable for deployment on edge devices. Oxford Nanopore Technology has also released MinION Mk1C based on the Jetson platform for real-time analysis of sequencing data. This article describes how to install and run Bonito Basecaller on Jetson for real-time sequencing data analysis and model training on edge devices.</div><div class=post-meta><span title='2025-01-25 21:00:00 +0000 UTC'>2025-01-25</span>&nbsp;·&nbsp;<span title='2025-01-26 21:53:35 +0800 +0800'>LastMod: 2025-01-26</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;sparktour&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://blog.sparktour.me/posts/2025/01/25/port-ont-bonito-koi-to-jetson-arm64/>中文</a></li></ul></div></header><figure class=entry-cover><img loading=eager src=https://assets.sparktour.me/img/blog/2025/port-ont-bonito-koi-to-jetson-arm64/cover.jpg alt="Bonito on Jetson Xavier NX"><figcaption>Bonito on Jetson Xavier NX</figcaption></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#ensure-cuda-dependencies aria-label="Ensure CUDA Dependencies">Ensure CUDA Dependencies</a></li><li><a href=#install-conda aria-label="Install Conda">Install Conda</a></li><li><a href=#install-bonito aria-label="Install Bonito">Install Bonito</a><ul><li><a href=#pytorch aria-label=PyTorch>PyTorch</a></li><li><a href=#ont-koi aria-label=ont-koi>ont-koi</a><ul><li><a href=#compile-so-file aria-label="Compile so file">Compile so file</a></li><li><a href=#install-ont-koi aria-label="Install ont-koi">Install ont-koi</a></li></ul></li><li><a href=#install-bonito-1 aria-label="Install Bonito">Install Bonito</a></li></ul></li><li><a href=#run-bonito aria-label="Run Bonito">Run Bonito</a></li></ul></div></details></div><div class=post-content><p><a href=https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/>Jetson</a> is an embedded AI computing platform launched by Nvidia. Compared to laptops and servers, Jetson has a smaller size and lower power consumption, making it suitable for deployment on edge devices. Oxford Nanopore Technology has also released <a href=https://nanoporetech.com/document/requirements/minion-mk1c-spec>MinION Mk1C</a> based on the Jetson platform for real-time analysis of sequencing data. This article describes how to install and run Bonito Basecaller on Jetson for real-time sequencing data analysis and model training on edge devices.</p><div class="callout callout-info"><p>This article is based on the <a href=https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-xavier-nx/>Jetson Xavier NX</a> development board running <a href=https://developer.nvidia.com/embedded/jetson-linux-r3521>Jetson Linux 35.2.1</a>. Other Jetson devices may require minor adjustments.</p><pre tabindex=0><code>$ uname -r
5.10.216-tegra

$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Sun_Oct_23_22:16:07_PDT_2022
Cuda compilation tools, release 11.4, V11.4.315
Build cuda_11.4.r11.4/compiler.31964100_0
</code></pre></div><h2 id=ensure-cuda-dependencies>Ensure CUDA Dependencies<a hidden class=anchor aria-hidden=true href=#ensure-cuda-dependencies>#</a></h2><p>Bonito Basecaller is a deep learning model based on CUDA, so it is necessary to ensure that CUDA dependencies are installed. During the installation of Jetson Linux using sdkmanager, you need to select the installation of the CUDA Toolkit to ensure that CUDA dependencies are installed. After entering the system, you can check the CUDA version using the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ nvcc --version
</span></span><span class=line><span class=cl>nvcc: NVIDIA <span class=o>(</span>R<span class=o>)</span> Cuda compiler driver
</span></span><span class=line><span class=cl>Copyright <span class=o>(</span>c<span class=o>)</span> 2005-2022 NVIDIA Corporation
</span></span><span class=line><span class=cl>Built on Sun_Oct_23_22:16:07_PDT_2022
</span></span><span class=line><span class=cl>Cuda compilation tools, release 11.4, V11.4.315
</span></span><span class=line><span class=cl>Build cuda_11.4.r11.4/compiler.31964100_0
</span></span></code></pre></div><h2 id=install-conda>Install Conda<a hidden class=anchor aria-hidden=true href=#install-conda>#</a></h2><p>To avoid conflicts with the system&rsquo;s Python environment, we can use Conda to create an independent Python environment. First, we create a Conda environment:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>conda create -n bonito-py38-081
</span></span><span class=line><span class=cl>conda activate bonito-py38-081
</span></span></code></pre></div><p>And install Python 3.8 in this environment:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>conda install <span class=nv>python</span><span class=o>=</span>3.8
</span></span></code></pre></div><h2 id=install-bonito>Install Bonito<a hidden class=anchor aria-hidden=true href=#install-bonito>#</a></h2><p>Observing the <code>requirements.txt</code> file of Bonito, we can see that Bonito depends on the following Python libraries:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># general requirements</span>
</span></span><span class=line><span class=cl>edlib
</span></span><span class=line><span class=cl>fast-ctc-decode
</span></span><span class=line><span class=cl>mappy
</span></span><span class=line><span class=cl>networkx     <span class=c1># required for py3.8 torch compatability</span>
</span></span><span class=line><span class=cl>numpy&lt;<span class=m>2</span>      <span class=c1># numpy~=2 is currently unreleased, it may have breaking changes</span>
</span></span><span class=line><span class=cl>pandas&lt;<span class=m>3</span>     <span class=c1># pandas~=3 is currently unreleased, it may have breaking changes</span>
</span></span><span class=line><span class=cl>parasail
</span></span><span class=line><span class=cl>pod5
</span></span><span class=line><span class=cl>pysam
</span></span><span class=line><span class=cl>python-dateutil
</span></span><span class=line><span class=cl>requests
</span></span><span class=line><span class=cl>toml
</span></span><span class=line><span class=cl>tqdm
</span></span><span class=line><span class=cl>wheel
</span></span><span class=line><span class=cl><span class=c1># specific requirements</span>
</span></span><span class=line><span class=cl><span class=nv>torch</span><span class=o>==</span>2.1.2
</span></span><span class=line><span class=cl><span class=c1># ont requirements</span>
</span></span><span class=line><span class=cl>ont-fast5-api
</span></span><span class=line><span class=cl>ont-koi
</span></span><span class=line><span class=cl>ont-remora
</span></span></code></pre></div><p>After testing by the author, the packages that have compatibility issues on the arm64 architecture are mainly focused on <code>torch</code> and <code>ont-koi</code>. Among them, PyTorch requires the installation of the torch package provided by NVIDIA, and ont-koi requires the porting of the dorado koi library.</p><h3 id=pytorch>PyTorch<a hidden class=anchor aria-hidden=true href=#pytorch>#</a></h3><p>According to Nvidia&rsquo;s <a href=https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform/index.html>documentation</a> and compatibility matrix <a href=https://docs.nvidia.com/deeplearning/frameworks/install-pytorch-jetson-platform-release-notes/pytorch-jetson-rel.html#pytorch-jetson-rel>table</a>, we can install version <code>2.1.0a</code> which is closest to <code>Pytorch 2.1.2</code> (Please select the appropriate PyTorch version according to the actual Jetpack Linux version):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Download the PyTorch wheel file</span>
</span></span><span class=line><span class=cl>wget https://developer.download.nvidia.com/compute/redist/jp/v512/pytorch/torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl
</span></span><span class=line><span class=cl><span class=c1># Install numpy and pandas</span>
</span></span><span class=line><span class=cl>pip install <span class=s2>&#34;numpy&lt;2&#34;</span> <span class=s2>&#34;pandas&lt;3&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># Install PyTorch</span>
</span></span><span class=line><span class=cl>pip install torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl
</span></span></code></pre></div><p>After installation, you can use the following command to determine that PyTorch has read the Jetson&rsquo;s GPU:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python3
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt;&gt;&gt; import torch
</span></span><span class=line><span class=cl>torch.cuda.get_device_name<span class=o>(</span>0<span class=o>)</span>
</span></span><span class=line><span class=cl>&gt;&gt;&gt; torch.cuda.get_device_name<span class=o>(</span>0<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;Xavier&#39;</span>
</span></span></code></pre></div><h3 id=ont-koi>ont-koi<a hidden class=anchor aria-hidden=true href=#ont-koi>#</a></h3><h4 id=compile-so-file>Compile so file<a hidden class=anchor aria-hidden=true href=#compile-so-file>#</a></h4><p>Since ONT only provides the koi library for the x86_64 architecture, we need to port the koi library to the arm64 architecture ourselves. After observation by the author, pre-compiled koi packages were found in Dorado&rsquo;s <a href=https://github.com/nanoporetech/dorado/blob/5c64f8c43d78bc3cf4d3842686792c21985ba055/cmake/Koi.cmake>Koi.cmake</a>. We can directly download and compile the so file. We first download the koi package from ONT:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Download the koi package</span>
</span></span><span class=line><span class=cl>wget https://cdn.oxfordnanoportal.com/software/analysis/libkoi-0.4.3-Linux-aarch64-cuda-11.4.tar.gz
</span></span><span class=line><span class=cl>tar -zxvf libkoi-0.4.3-Linux-aarch64-cuda-11.4.tar.gz
</span></span></code></pre></div><p>Then, create a <code>setup.py</code> in the folder:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>cffi</span> <span class=kn>import</span> <span class=n>FFI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ffi</span> <span class=o>=</span> <span class=n>FFI</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_koi</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;include/koi.h&#34;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>header_content</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>ffi</span><span class=o>.</span><span class=n>cdef</span><span class=p>(</span><span class=n>header_content</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>ffi</span><span class=o>.</span><span class=n>set_source</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;koi._runtime&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;#include &lt;koi.h&gt;&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>include_dirs</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;./include&#39;</span><span class=p>],</span>  <span class=c1># include</span>
</span></span><span class=line><span class=cl>        <span class=n>library_dirs</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;./lib&#39;</span><span class=p>],</span>      <span class=c1># lib</span>
</span></span><span class=line><span class=cl>        <span class=n>libraries</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;koi&#39;</span><span class=p>],</span>           <span class=c1># link libkoi.a</span>
</span></span><span class=line><span class=cl>        <span class=n>extra_link_args</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;-Wl,-rpath,$ORIGIN/lib&#39;</span><span class=p>]</span> 
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>ffi</span><span class=o>.</span><span class=n>compile</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>build_koi</span><span class=p>()</span>
</span></span></code></pre></div><p>And run the script. After compilation, we can find the <code>_runtime.cpython-38-aarch64-linux-gnu.so</code> file in the current directory.</p><div class="callout callout-info">The author also provides a pre-compiled so file for readers to use: <a href=https://assets.sparktour.me/img/blog/2025/port-ont-bonito-koi-to-jetson-arm64/_runtime.cpython-38-aarch64-linux-gnu.so.tar.gz>https://assets.sparktour.me/img/blog/2025/port-ont-bonito-koi-to-jetson-arm64/_runtime.cpython-38-aarch64-linux-gnu.so.tar.gz</a></div><h4 id=install-ont-koi>Install ont-koi<a hidden class=anchor aria-hidden=true href=#install-ont-koi>#</a></h4><p>Since the koi architecture is different, we need to install ont-koi manually:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># We first download the x86_64 architecture whl package</span>
</span></span><span class=line><span class=cl>wget https://files.pythonhosted.org/packages/fe/68/4bb9241c65d6c0c51b6ae366b5dedda2e8a41ba17d2559079ee8811808ef/ont_koi-0.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Unzip the wheel file (a wheel is essentially a zip file)</span>
</span></span><span class=line><span class=cl>unzip ont_koi-0.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -d ont_koi_temp
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Find the Python site-packages directory</span>
</span></span><span class=line><span class=cl><span class=nv>SITE_PACKAGES</span><span class=o>=</span><span class=k>$(</span>python -c <span class=s2>&#34;import site; print(site.getsitepackages()[0])&#34;</span><span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Copy the files directly to site-packages (but be sure to back up first)</span>
</span></span><span class=line><span class=cl>cp -r ont_koi_temp/koi <span class=nv>$SITE_PACKAGES</span>/
</span></span><span class=line><span class=cl>cp -r ont_koi_temp/ont_koi.libs <span class=nv>$SITE_PACKAGES</span>/
</span></span><span class=line><span class=cl>cp -r ont_koi_temp/ont_koi-0.4.4.dist-info <span class=nv>$SITE_PACKAGES</span>/
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Replace the so file</span>
</span></span><span class=line><span class=cl>cp /path/to/your/new/_runtime.cpython-38-aarch64-linux-gnu.so <span class=nv>$SITE_PACKAGES</span>/koi/_runtime.abi3.so
</span></span></code></pre></div><h3 id=install-bonito-1>Install Bonito<a hidden class=anchor aria-hidden=true href=#install-bonito-1>#</a></h3><p>We first download the Bonito source code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/nanoporetech/bonito
</span></span><span class=line><span class=cl><span class=nb>cd</span> bonito
</span></span></code></pre></div><p>Comment out the dependencies of <code>torch</code> and <code>ont-koi</code> in <code>requirements.txt</code>, and then install Bonito:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install .
</span></span></code></pre></div><h2 id=run-bonito>Run Bonito<a hidden class=anchor aria-hidden=true href=#run-bonito>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>bonito basecaller dna_r9.4.1_e8_sup@v3.3 --reference reference.mmi /data/reads &gt; basecalls.bam
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.sparktour.me/en/tags/bonito/>Bonito</a></li></ul><nav class=paginav><a class=prev href=https://blog.sparktour.me/en/posts/2025/12/31/2025-in-photos/><span class=title>« Prev</span><br><span>2025 in Photos</span>
</a><a class=next href=https://blog.sparktour.me/en/posts/2024/12/31/2024-in-photos/><span class=title>Next »</span><br><span>2024 in Photos</span></a></nav></footer><div class="callout callout-default"><img src=https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by-nc-sa.svg alt="CC BY-NC-SA 4.0" style=width:88px;height:31px><p style=font-size:12px>Licensed Under <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank rel=noopener>CC BY-NC-SA 4.0</a></p><p style=font-size:12px>Post Link: <a href=https://blog.sparktour.me/en/posts/2025/01/25/port-ont-bonito-koi-to-jetson-arm64/ target=_blank rel=noopener>https://blog.sparktour.me/en/posts/2025/01/25/port-ont-bonito-koi-to-jetson-arm64/</a></p></div><script src=https://s4.zstatic.net/ajax/libs/disqusjs/3.0.2/disqusjs.es2015.umd.min.js crossorigin=anonymous></script><div id=disqusjs></div><script>const disqusjs=new DisqusJS({shortname:"sparktour",siteName:"",identifier:"",url:"",title:"",api:"https://disqus.com/api/",apikey:"QhJnXpS5igyHkjYQ91XYC4dSAuEJYAEsHX8hjLrRc1HU9AApOHLrqkwwIp2sKOLk",admin:"",adminLabel:""});disqusjs.render(document.getElementById("disqusjs"))</script></article></main><footer class=footer><span>&copy; 2026 <a href=https://blog.sparktour.me/en/>Sparktour’s Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>